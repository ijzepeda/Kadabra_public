{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the string text after removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "def tokenization(text):\n",
    "    # REMOVING PUNCTUATIONS\n",
    "    inputText = re.sub(r'[^a-zA-Z]', '', inputText)\n",
    "    lowerString = inputText.lower()\n",
    "    wordToknized = word_tokenize(lowerString)\n",
    "    return wordToknized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def removeStopwords(tokens):\n",
    "    wordList = stopwords.words('english')\n",
    "    filtered_words = [word for word in tokens if word not in wordList]\n",
    "    \n",
    "    return filtered_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "def wordlemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    noun = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    verb = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "    adverb = ['RB', 'RBR', 'RBS' ]\n",
    "    adjective = ['JJ','JJR','JJS']\n",
    "    for word in tokens:\n",
    "        postTag = dict(pos_tag([word]))\n",
    "        if postTag[word] in noun:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, pos='n'))\n",
    "        elif postTag[word] in verb:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "        elif postTag[word] in adjective:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, pos='a'))\n",
    "        elif postTag[word] in adverb:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, pos='r'))\n",
    "        else:\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word))\n",
    "    \n",
    "    return lemmatized_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check spelling of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def correct_spelling(text):\n",
    "    blob = TextBlob(text)\n",
    "    corrected_text = str(blob.correct())\n",
    "    return corrected_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect Language using langID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langid\n",
    "from langcodes import Language\n",
    "\n",
    "def detect_language_langID(text):\n",
    "    detected_languages = {}\n",
    "    for word in text:\n",
    "        language = langid.classify(word)[0]\n",
    "        detected_languages[word] = Language.get(language).display_name()\n",
    "        print(word, ':',detected_languages[word])\n",
    "\n",
    "    return detected_languages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counEnglishwords(text):\n",
    "    engCount = 0\n",
    "    length = len(text)\n",
    "    for word, lang in text.items():\n",
    "        if lang == 'English':\n",
    "            engCount +=1\n",
    "    total_per = (engCount/length)*100\n",
    "\n",
    "    return total_per"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline to perform textpreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : English\n",
      "first : English\n",
      "sentence : English\n",
      "english : Italian\n",
      "second : English\n",
      "sentence : English\n",
      "japanese : English\n",
      "third : English\n",
      "sentence : English\n",
      "include : English\n",
      "chinese : English\n",
      "greet : English\n",
      "9\n",
      "10\n",
      "90.0\n",
      "90.0\n"
     ]
    }
   ],
   "source": [
    "# text = \"In this text, the first sentence is in English, the second sentence is in Japanese, the third sentence includes a Chinese greeting\"\n",
    "\n",
    "# def main():\n",
    "#     pipeline_functions = [correct_spelling, tokenization, removeStopwords, wordlemmatize, detect_language_langID, counEnglishwords]\n",
    "\n",
    "#     processed_data = text\n",
    "#     for func in pipeline_functions:\n",
    "#         processed_data = func(processed_data)\n",
    "\n",
    "#     final_result = processed_data\n",
    "#     print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\prabh\\OneDrive\\Documents\\Lambton\\Semester 2\\Natural Language Processing\\reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>position</th>\n",
       "      <th>midb_id</th>\n",
       "      <th>movie</th>\n",
       "      <th>spoilers</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>user</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>False</td>\n",
       "      <td>1/10</td>\n",
       "      <td>Pale imitation of better films</td>\n",
       "      <td>sbaradell</td>\n",
       "      <td>10 July 2003</td>\n",
       "      <td>Three words: \"Cool Hand Luke.\"  Same film, don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>tt0111161</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>False</td>\n",
       "      <td>1/10</td>\n",
       "      <td>Didactic and overlong</td>\n",
       "      <td>arthur_pewtey</td>\n",
       "      <td>19 November 2000</td>\n",
       "      <td>Another one of those overlong morally right-on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  position    midb_id                     movie  spoilers rating  \\\n",
       "0           0         0  tt0111161  The Shawshank Redemption     False   1/10   \n",
       "1           1         0  tt0111161  The Shawshank Redemption     False   1/10   \n",
       "\n",
       "                            title           user              date  \\\n",
       "0  Pale imitation of better films      sbaradell      10 July 2003   \n",
       "1           Didactic and overlong  arthur_pewtey  19 November 2000   \n",
       "\n",
       "                                             content  \n",
       "0  Three words: \"Cool Hand Luke.\"  Same film, don...  \n",
       "1  Another one of those overlong morally right-on...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDf = df['content'][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Three words: \"Cool Hand Luke.\"  Same film, done better, done earlier.  For that matter, is this film any better than other Steven King \"novelettes\" such as \"Stand By Me\"? All in all, it probably ranks a 6 or a 7, but since people on this site have lost their minds as regards this film, I give it a 1 in one man\\'s attempt at sanity.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed = list(newDf)\n",
    "processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "犬 : Chinese\n",
      "árbol : Hungarian\n",
      "mode : English\n",
      "chief : English\n",
      "árbol : Hungarian\n",
      "كلب : Arabic\n",
      "树 : Chinese\n",
      "co : English\n",
      "bonjour : English\n",
      "undo : Spanish\n",
      "мир : Russian\n",
      "hand : English\n",
      "привет : Bulgarian\n",
      "hold : English\n",
      "co : English\n",
      "perry : English\n",
      "bonjour : English\n",
      "albert : English\n",
      "árvore : Hungarian\n",
      "albert : English\n",
      "árbol : Hungarian\n",
      "chief : English\n",
      "árvore : Hungarian\n",
      "raum : German\n",
      "co : English\n",
      "undo : Spanish\n",
      "قطة : Arabic\n",
      "cane : English\n",
      "hand : English\n",
      "hold : English\n",
      "شجرة : Arabic\n",
      "мир : Russian\n",
      "welt : German\n",
      "кошка : Serbian\n",
      "Final Processed Text has 40.909090909090914 % of english words\n"
     ]
    }
   ],
   "source": [
    "pipeline_functions = [correct_spelling, tokenization, removeStopwords, wordlemmatize, detect_language_langID, counEnglishwords]\n",
    "\n",
    "#processed_data = processed[0]\n",
    "processed_data = \"犬 árbol 世界 你好 monde chien árbol كلب 树 cão bonjour mundo мир hund привет hola cão perro bonjour albero ciao árvore albero árbol chien árvore 你好 baum cão mundo قطة cane 你好 hund hola شجرة arbre мир welt кошка\"\n",
    "for func in pipeline_functions:\n",
    "    processed_data = func(processed_data)\n",
    "\n",
    "final_result = processed_data\n",
    "print('Final Processed Text has {} % of english words'.format(final_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
